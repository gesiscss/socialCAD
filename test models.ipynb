{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load trained models and test on all data\n",
    "### save predictions as pkls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "constructs = ['sentiment', 'sexism', 'hatespeech']\n",
    "\n",
    "test_sets = {\"sentiment\" : [\n",
    "                            \"original\",\n",
    "                            \"kaggle\",\n",
    "                            ],\n",
    "             \"sexism\": [\n",
    "                             \"original\",\n",
    "                             \"exist\",\n",
    "                       ],\n",
    "             \"hatespeech\" : [\n",
    "                             \"original\",\n",
    "                            \"hateval\"]\n",
    "            }\n",
    "\n",
    "domain_mapping = {'original' : 'ID',\n",
    "                 'kaggle' : 'OOD',\n",
    "                 'exist' : 'OOD',\n",
    "                 'hateval' : 'OOD'}\n",
    "\n",
    "adv_test_sets = ['adv_inv', 'adv_swap']\n",
    "\n",
    "labels = {'sentiment' : {'positive': 1, 'negative' : 0},\n",
    "          'sexism' : {'sexist' : 1, 'non-sexist' : 0},\n",
    "          'hatespeech' : {'hate' : 1, 'not hate' : 0}\n",
    "                          }\n",
    "\n",
    "runs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {}\n",
    "\n",
    "for construct in constructs:\n",
    "    if construct not in classifiers:\n",
    "        classifiers[construct] = {}\n",
    "    for model in models:\n",
    "        if model not in classifiers[construct]:\n",
    "            classifiers[construct][model] = {}\n",
    "        for mode in modes:\n",
    "            if mode not in classifiers[construct][model]:\n",
    "                classifiers[construct][model][mode] = []\n",
    "            for run in range(runs):\n",
    "                if model == 'bert':\n",
    "                    classifiers[construct][model][mode].append(load_model('../ml_models/%s_%s_%s_%d.joblib' %(construct,\n",
    "                                                                                                      model,\n",
    "                                                                                                      mode,\n",
    "                                                                                                      run))) \n",
    "                else:\n",
    "                    classifiers[construct][model][mode].append(load('../ml_models/%s_%s_%s_%d.joblib' %(construct,\n",
    "                                                                                            model,\n",
    "                                                                                            mode,\n",
    "                                                                                            run)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(estimators, construct, test_sets, labels = {'positive' : 1, 'negative' : 0},\n",
    "                      keep_neutral = False):\n",
    "    all_test_data = pd.DataFrame()\n",
    "    for test in test_sets:\n",
    "        dists = []\n",
    "        data = pd.read_csv(\"../data/data/%s/test/%s.csv\" %(construct, test), sep = \"\\t\")\n",
    "        if not keep_neutral:\n",
    "            data = data[data[construct].isin(labels)].reset_index()\n",
    "        \n",
    "        data[construct] = data[construct].map(labels)\n",
    "        print(len(data))\n",
    "        \n",
    "        for n, mode in enumerate([True, False]):\n",
    "            if mode:\n",
    "                mode_ = 'Counterfactual'\n",
    "            else:\n",
    "                mode_ = 'Non-counterfactual'\n",
    "            proba = estimators[mode][0].predict_proba(data['text'])\n",
    "            y_pred = estimators[mode][0].predict(data['text'])\n",
    "            df = pd.DataFrame(proba, columns = ['neg prob', 'pos prob'])\n",
    "            df['_id'] = range(len(df))\n",
    "            df['pred'] = y_pred\n",
    "            df[construct] = data[construct]\n",
    "            \n",
    "            # for getting all predictions\n",
    "            data['%s pred' %(mode)] = df['pred']\n",
    "            data['%s pred proba' %(mode)] = df['pos prob']\n",
    "            data['%s correct' %(mode)] = ['yes' if row[construct] == row['%s pred' %(mode)] else 'no'\\\n",
    "                                  for i, row in data.iterrows()] \n",
    "            \n",
    "            \n",
    "            dists.append(df['pos prob'])\n",
    "            \n",
    "            data['dataset'] = test\n",
    "            \n",
    "        data = data[['index', 'text', construct, 'dataset', 'True pred',\n",
    "                     'True pred proba', 'False pred', 'False pred proba',\n",
    "                     'True correct', 'False correct']]        \n",
    "        all_test_data = all_test_data.append(data)\n",
    "\n",
    "        print(test)\n",
    "        print(stats.ttest_ind(dists[0], dists[1], equal_var = False))\n",
    "        print()\n",
    "    \n",
    "    return all_test_data\n",
    "\n",
    "\n",
    "#run once and save results\n",
    "for construct in constructs:\n",
    "        all_test_data[construct] = {}\n",
    "        print(construct)\n",
    "        for model in models:\n",
    "            all_test_data[construct][model] = predict(classifiers[construct][model], construct,\n",
    "                                                    test_sets = test_sets[construct],\n",
    "                                                    labels = labels[construct])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for adversarial examples but also test on their original counterparts \n",
    "# to prevent data size discrepencies\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for construct in constructs:\n",
    "    for run in range(runs):\n",
    "        for model_name in models:\n",
    "            for mode in [False, True]:\n",
    "                for test_type in adv_test_types:\n",
    "                    data = pd.read_csv(DATAPATH+\"%s.csv\" %(test_type), sep = '\\t')\n",
    "                \n",
    "                    print()\n",
    "                    print(construct, model_name, mode, test_type)\n",
    "                    \n",
    "                    # first the original examples\n",
    "                    true, pred, cr = test(trained_models[construct][model_name][mode][run],\n",
    "                                          data, construct, test = test_type + \" original\",\n",
    "                                          labels = labels[construct], text_column = 'original')\n",
    "                    all_results.append(get_results(cr, true, pred,\n",
    "                                              method = model_name,\n",
    "                                              mode = mode,\n",
    "                                              construct = construct,     \n",
    "                                              labels = {str(v): k for k, v in labels[construct].items()},\n",
    "                                              dataset = test_type + \" original\"))\n",
    "                    \n",
    "                    # second the adversarial examples\n",
    "                    true, pred, cr = test(trained_models[construct][model_name][mode][run],\n",
    "                                          data, construct, test = test_type,\n",
    "                                          labels = labels[construct])\n",
    "                    all_results.append(get_results(cr, true, pred,\n",
    "                                              method = model_name,\n",
    "                                              mode = mode,\n",
    "                                              construct = construct,     \n",
    "                                              labels = {str(v): k for k, v in labels[construct].items()},\n",
    "                                              dataset = test_type))\n",
    "\n",
    "results = {}\n",
    "result_df = pd.DataFrame(all_results)\n",
    "result_df = result_df.rename({'1 Class F1': 'Pos F1'}, axis=1) \n",
    "for construct in constructs:\n",
    "    result_df_ = result_df[result_df['construct'] == construct]\n",
    "    results[construct] = result_df_.groupby(['construct','method', 'dataset', 'mode'])[['Pos F1', 'Macro F1']].mean().unstack()                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle that dict\n",
    "import pickle \n",
    "\n",
    "# with open(\"../results/result_pkls/adversarial_results.pkl\", 'wb+') as f:\n",
    "#     pickle.dump(all_test_data, f)\n",
    "\n",
    "# with open(\"../results/result_pkls/adversarial_results.pkl2\", 'wb+') as f:\n",
    "#     pickle.dump(all_test_data, f, protocol=2)\n",
    "    \n",
    "    \n",
    "# read results dict\n",
    "\n",
    "\n",
    "with open(\"../results/result_pkls/in_out_domain_results.pkl\", 'rb') as handle:\n",
    "    all_test_data = pickle.load(handle)\n",
    "    \n",
    "    \n",
    "with open(\"../results/result_pkls/adversarial_results.pkl\", 'rb') as handle:\n",
    "    adv_test_data = pickle.load(handle)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
